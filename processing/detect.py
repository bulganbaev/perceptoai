import cv2
import numpy as np
import os
import time
import hailo_platform as hp


class DepthEstimator:
    def __init__(self, calib_path="data/calibration/calibration_data.npz", use_hailo=True,
                 hef_path="data/models/yolov8m_pose.hef"):
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ ÐºÐ°Ð»Ð¸Ð±Ñ€Ð¾Ð²ÐºÐ¸
        calib_data = np.load(calib_path)
        self.mtxL, self.distL = calib_data["mtxL"], calib_data["distL"]
        self.mtxR, self.distR = calib_data["mtxR"], calib_data["distR"]
        self.R, self.T = calib_data["R"], calib_data["T"]

        self.R1, self.R2, self.P1, self.P2, self.Q, _, _ = cv2.stereoRectify(
            self.mtxL, self.distL, self.mtxR, self.distR, (1920, 1080), self.R, self.T, alpha=1)

        self.mapL1, self.mapL2 = cv2.initUndistortRectifyMap(self.mtxL, self.distL, self.R1, self.P1, (1920, 1080),
                                                             cv2.CV_16SC2)
        self.mapR1, self.mapR2 = cv2.initUndistortRectifyMap(self.mtxR, self.distR, self.R2, self.P2, (1920, 1080),
                                                             cv2.CV_16SC2)

        self.model_w, self.model_h = 640, 640
        self.use_hailo = use_hailo
        if use_hailo:
            self.vdevice = hp.VDevice()
            self.hef = hp.HEF(hef_path)
            configure_params = hp.ConfigureParams.create_from_hef(self.hef, interface=hp.HailoStreamInterface.PCIe)
            self.network_groups = self.vdevice.configure(self.hef, configure_params)
            self.configured_network = self.network_groups[0]

            self.input_vstream_infos = self.configured_network.get_input_vstream_infos()
            self.output_vstream_infos = self.configured_network.get_output_vstream_infos()

            self.input_vstreams_params = hp.InputVStreamParams.make_from_network_group(self.configured_network)
            self.output_vstreams_params = hp.OutputVStreamParams.make_from_network_group(self.configured_network,
                                                                                         format_type=hp.FormatType.FLOAT32)

            self.infer_vstreams = hp.InferVStreams(self.configured_network, self.input_vstreams_params,
                                                   self.output_vstreams_params)

            print("âœ… Hailo-8 ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½. YOLO11s Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½.")

    def preprocess_stereo(self, imgL, imgR):
        """
        ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÑ‚ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð¸ Ñ†ÐµÐ½Ñ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¸Ñ… Ð½Ð° Ñ‡ÐµÑ€Ð½Ð¾Ð¼ Ñ„Ð¾Ð½Ðµ.
        """

        def resize_and_pad(image):
            img_h, img_w, _ = image.shape
            scale = min(self.model_w / img_w, self.model_h / img_h)
            new_img_w, new_img_h = int(img_w * scale), int(img_h * scale)
            image_resized = cv2.resize(image, (new_img_w, new_img_h))

            padded_image = np.zeros((self.model_h, self.model_w, 3), dtype=np.uint8)
            pasted_w = (self.model_w - new_img_w) // 2
            pasted_h = (self.model_h - new_img_h) // 2
            padded_image[pasted_h:pasted_h + new_img_h, pasted_w:pasted_w + new_img_w, :] = image_resized
            return padded_image

        imgL_padded = resize_and_pad(imgL)
        imgR_padded = resize_and_pad(imgR)

        return imgL_padded, imgR_padded

    def detect_objects(self, img):
        """
        Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸ÑŽ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· YOLO11s Ð½Ð° Hailo Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ñ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚.
        """
        img_resized = cv2.resize(img, (640, 640))  # Ð Ð°Ð·Ð¼ÐµÑ€ Ð²Ñ…Ð¾Ð´Ð° Ð´Ð»Ñ YOLO
        img_resized = np.ascontiguousarray(img_resized.astype(np.uint8)).reshape(1, 640, 640, 3)

        input_data = {"yolov8m_pose/input_layer1": img_resized}

        with self.infer_vstreams as infer_pipeline:
            with self.configured_network.activate():
                output_data = infer_pipeline.infer(input_data)

        boxes = output_data["yolov8m_pose/conv59"]  # Bounding boxes
        boxes = np.squeeze(boxes)  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ batch dim (1, 80, 80, 64) â†’ (80, 80, 64)

        # YOLO Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ bbox Ð² Ð²Ð¸Ð´Ðµ feature map, Ð½Ð°Ð¼ Ð½ÑƒÐ¶Ð½Ð¾ Ð²Ñ‹Ñ‚Ð°Ñ‰Ð¸Ñ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½ÑƒÐ¶Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        num_classes = 1  # YOLOv8 Pose Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÐºÐ»Ð°ÑÑÑ‹ (0 - Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº)
        num_bbox_params = 5  # x_center, y_center, width, height, confidence

        # ÐžÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ bbox-Ð´Ð°Ð½Ð½Ñ‹Ðµ
        bboxes = boxes[..., :num_bbox_params]  # Ð‘ÐµÑ€Ñ‘Ð¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 5 ÐºÐ°Ð½Ð°Ð»Ð¾Ð²

        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð±Ð¾ÐºÑÑ‹ Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¸Ð¼ confidence
        threshold = 0.5  # ÐœÐ¾Ð¶ÐµÑˆÑŒ Ð¼ÐµÐ½ÑÑ‚ÑŒ, ÐµÑÐ»Ð¸ Ð±Ð¾ÐºÑÐ¾Ð² Ð¼Ð°Ð»Ð¾/Ð¼Ð½Ð¾Ð³Ð¾
        filtered_boxes = bboxes[bboxes[..., 4] > threshold]

        # YOLO Ð²Ñ‹Ð´Ð°Ñ‘Ñ‚ x_center, y_center, width, height Ð² Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ð°Ñ…
        # ÐÐ°Ð¼ Ð½Ð°Ð´Ð¾ Ð¿ÐµÑ€ÐµÐ²ÐµÑÑ‚Ð¸ Ð¸Ñ… Ð² Ð¿Ð¸ÐºÑÐµÐ»Ð¸
        img_width, img_height = 640, 640  # Ð Ð°Ð·Ð¼ÐµÑ€ Ð²Ñ…Ð¾Ð´Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸ (ÑƒÐ±ÐµÐ´Ð¸ÑÑŒ, Ñ‡Ñ‚Ð¾ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÐµÑ‚!)
        filtered_boxes[:, 0] *= img_width  # x_center â†’ Ð¿Ð¸ÐºÑÐµÐ»Ð¸
        filtered_boxes[:, 1] *= img_height  # y_center â†’ Ð¿Ð¸ÐºÑÐµÐ»Ð¸
        filtered_boxes[:, 2] *= img_width  # width â†’ Ð¿Ð¸ÐºÑÐµÐ»Ð¸
        filtered_boxes[:, 3] *= img_height  # height â†’ Ð¿Ð¸ÐºÑÐµÐ»Ð¸

        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð· (x_center, y_center, width, height) â†’ (x1, y1, x2, y2)
        filtered_boxes[:, 0] -= filtered_boxes[:, 2] / 2  # x1 = x_center - width/2
        filtered_boxes[:, 1] -= filtered_boxes[:, 3] / 2  # y1 = y_center - height/2
        filtered_boxes[:, 2] += filtered_boxes[:, 0]  # x2 = x1 + width
        filtered_boxes[:, 3] += filtered_boxes[:, 1]  # y2 = y1 + height

        # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð±Ð¾ÐºÑÑ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð²Ñ‹Ñ…Ð¾Ð´ÑÑ‚ Ð·Ð° Ð³Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
        filtered_boxes = filtered_boxes[
            (filtered_boxes[:, 0] >= 0) & (filtered_boxes[:, 1] >= 0) &
            (filtered_boxes[:, 2] <= img_width) & (filtered_boxes[:, 3] <= img_height)
            ]

        print(f"ðŸ“Œ ÐžÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¾ {filtered_boxes.shape[0]} Ð±Ð¾ÐºÑÐ¾Ð² Ð¿Ð¾ÑÐ»Ðµ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ†Ð¸Ð¸:")
        print(filtered_boxes)

        return filtered_boxes

    def draw_boxes(self, image, boxes, color=(0, 255, 0)):
        for box in boxes:
            x1, y1, x2, y2, conf = box
            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
            cv2.putText(image, f"{conf:.2f}", (int(x1), int(y1) - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        return image

    def compute_detection(self, imgL_path, imgR_path):
        start_time = time.time()

        imgL = cv2.imread(imgL_path)
        imgR = cv2.imread(imgR_path)

        if imgL is None or imgR is None:
            raise ValueError("ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹! ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð¿ÑƒÑ‚Ð¸.")

        # âœ… Ð”ÐµÐ»Ð°ÐµÐ¼ Ñ€ÐµÐºÑ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸ÑŽ
        imgL_rect = cv2.remap(imgL, self.mapL1, self.mapL2, cv2.INTER_LINEAR)
        imgR_rect = cv2.remap(imgR, self.mapR1, self.mapR2, cv2.INTER_LINEAR)

        # âœ… ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ñ†ÐµÐ½Ñ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
        imgL_padded, imgR_padded = self.preprocess_stereo(imgL_rect, imgR_rect)

        # âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð¿Ð¾ÑÐ»Ðµ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³Ð° Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
        cv2.imwrite("data/images/processed_left.png", imgL_padded)
        cv2.imwrite("data/images/processed_right.png", imgR_padded)
        print("âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ: processed_left.png, processed_right.png")

        # âœ… Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ YOLO Ð½Ð° Ð¾Ð±ÐµÐ¸Ñ… ÐºÐ°Ð¼ÐµÑ€Ð°Ñ…
        detections_left = self.detect_objects(imgL_padded)
        detections_right = self.detect_objects(imgR_padded)



        imgL_drawn = self.draw_boxes(imgL_padded.copy(), detections_left)
        cv2.imwrite("data/images/detected_left.png", imgL_drawn)

        imgR_drawn = self.draw_boxes(imgL_padded.copy(), detections_right)
        cv2.imwrite("data/images/detected_right.png", imgR_drawn)

        print("âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸: detected_left.png, detected_right.png")

        elapsed_time = time.time() - start_time
        print(f"â± Ð’Ñ€ÐµÐ¼Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ: {elapsed_time:.4f} ÑÐµÐº")


if __name__ == "__main__":
    detector = DepthEstimator(use_hailo=True)
    detector.compute_detection("data/images/left/left_00.jpg", "data/images/right/right_00.jpg")
