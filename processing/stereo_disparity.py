import cv2
import time
import numpy as np
from cam.camera_driver import CameraDriver
from processing.hailo_detection import HailoInference, Processor

# === 1. –ó–ê–ì–†–£–ó–ö–ê –ü–ê–†–ê–ú–ï–¢–†–û–í –ö–ê–õ–ò–ë–†–û–í–ö–ò ===
calib_data = np.load("data/calibration/calibration_data.npz")

mtxL = calib_data["mtxL"]
distL = calib_data["distL"]
mtxR = calib_data["mtxR"]
distR = calib_data["distR"]
R = calib_data["R"]
T = calib_data["T"]

BASELINE = abs(T[0][0])  # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –∫–∞–º–µ—Ä–∞–º–∏ (–º–º)
FOCAL_LENGTH = mtxL[0, 0]  # –§–æ–∫—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ –ø–∏–∫—Å–µ–ª—è—Ö

print(f"üîß –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: baseline={BASELINE:.2f}mm, focal={FOCAL_LENGTH:.2f}px")


def compute_disparity(left_box, right_point):
    """–í—ã—á–∏—Å–ª—è–µ—Ç disparity (—Ä–∞–∑–Ω–∏—Ü—É X) –º–µ–∂–¥—É bbox —Å–ª–µ–≤–∞ –∏ —Ç–æ—á–∫–æ–π —Å–ø—Ä–∞–≤–∞."""
    center_L_x = (left_box[1] + left_box[3]) // 2  # –¶–µ–Ω—Ç—Ä X bbox –≤ –ª–µ–≤–æ–π –∫–∞–º–µ—Ä–µ
    center_R_x = right_point[0]  # X-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Ç–æ—á–∫–∏ –≤ –ø—Ä–∞–≤–æ–π –∫–∞–º–µ—Ä–µ

    disparity = max(1, abs(center_L_x - center_R_x))  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    return disparity


def compute_depth(left_boxes, right_points):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ disparity."""
    depths = []
    for left_box, right_pt in zip(left_boxes, right_points):
        disparity = compute_disparity(left_box, right_pt)
        depth = (FOCAL_LENGTH * BASELINE) / disparity  # –ì–ª—É–±–∏–Ω–∞ –≤ –º–º
        depths.append((left_box[1], left_box[0], depth))  # (X, Y, Depth)
    return depths


def track_optical_flow(frame_left, frame_right, left_boxes):
    """–û–ø—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ç–æ–∫ Lucas-Kanade –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–æ—á–µ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –ø—Ä–∞–≤–æ–π –∫–∞–º–µ—Ä–µ."""
    gray_left = cv2.cvtColor(frame_left, cv2.COLOR_BGR2GRAY)
    gray_right = cv2.cvtColor(frame_right, cv2.COLOR_BGR2GRAY)

    left_pts = np.array([[(x1 + x2) // 2, (y1 + y2) // 2] for (y1, x1, y2, x2) in left_boxes], dtype=np.float32)

    if len(left_pts) == 0:
        return []

    right_pts, status, _ = cv2.calcOpticalFlowPyrLK(gray_left, gray_right, left_pts, None)

    matched_points = []
    for i, (new, status_flag) in enumerate(zip(right_pts, status)):
        if status_flag:
            matched_points.append(new)  # –ù–∞–π–¥–µ–Ω–Ω–∞—è —Ç–æ—á–∫–∞ –≤ –ø—Ä–∞–≤–æ–π –∫–∞–º–µ—Ä–µ

    return matched_points


# === 2. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –î–ï–¢–ï–ö–¶–ò–ò ===
inf = HailoInference('data/models/yolov11s.hef', 'data/labels/coco.txt')
proc = Processor(inf, conf=0.5)

# === 3. –ó–ê–ü–£–°–ö –ö–ê–ú–ï–† ===
cam_left = CameraDriver(camera_id=0)
cam_right = CameraDriver(camera_id=1)

cam_left.start_camera()
cam_right.start_camera()

print("üé• –ó–∞–ø—É—Å–∫ —Å—Ç—Ä–∏–º–∞. –ù–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞.")

try:
    while True:
        frame_left = cam_left.get_frame()
        frame_right = cam_right.get_frame()

        if frame_left is not None and frame_right is not None:
            # –ó–∞–ø—É—Å–∫ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –ª–µ–≤–æ–π –∫–∞–º–µ—Ä—ã
            detections = proc.process([frame_left])

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ class=0 (–æ–±—ã—á–Ω–æ —ç—Ç–æ 'person' –≤ COCO)
            for result in detections:
                filtered_boxes = []
                filtered_scores = []
                filtered_classes = []

                for i, class_id in enumerate(result['detection_classes']):
                    if class_id == 0:  # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ class=0
                        filtered_boxes.append(result['absolute_boxes'][i])
                        filtered_scores.append(result['detection_scores'][i])
                        filtered_classes.append(class_id)

                # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Ä–∏—Å—É–µ–º –±–æ–∫—Å—ã —Ç–æ–ª—å–∫–æ –¥–ª—è class=0
                result.update({
                    'absolute_boxes': filtered_boxes,
                    'detection_classes': filtered_classes,
                    'detection_scores': filtered_scores
                })

                frame_left = proc.label_loader.draw_boxes(result)

                # === 4. –û–ü–¢–ò–ß–ï–°–ö–ò–ô –ü–û–¢–û–ö –î–õ–Ø –ü–û–ò–°–ö–ê –¢–û–ß–ï–ö –í –ü–†–ê–í–û–ô –ö–ê–ú–ï–†–ï ===
                right_points = track_optical_flow(frame_left, frame_right, filtered_boxes)

                # === 5. –í–´–ß–ò–°–õ–ï–ù–ò–ï –ì–õ–£–ë–ò–ù–´ ===
                depth_results = compute_depth(filtered_boxes, right_points)

                # === 6. –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –ì–õ–£–ë–ò–ù–´ ===
                for x, y, d in depth_results:
                    cv2.putText(frame_left, f"Depth: {d:.1f} mm", (x, y),
                                cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ª–µ–≤—ã–π –∏ –ø—Ä–∞–≤—ã–π –∫–∞–¥—Ä
            combined_frame = cv2.hconcat([frame_left, frame_right])

            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ–∫–Ω–∞
            cv2.namedWindow("Stereo Stream", cv2.WINDOW_NORMAL)
            cv2.resizeWindow("Stereo Stream", 1920, 1080)
            cv2.imshow("Stereo Stream", combined_frame)

        # –í—ã—Ö–æ–¥ –ø–æ 'q'
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

except KeyboardInterrupt:
    print("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ç–æ–∫–∞...")

# –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É –∫–∞–º–µ—Ä
cam_left.stop_camera()
cam_right.stop_camera()
cv2.destroyAllWindows()
print("‚úÖ –ü–æ—Ç–æ–∫ –∑–∞–≤–µ—Ä—à—ë–Ω.")
